## ðŸ“˜ Documento de Arquitectura del Proyecto Integrador de Big Data

### 1. IntroducciÃ³n y DescripciÃ³n Global de la Arquitectura

El proyecto integra tres fases principales en un entorno simulado de Big Data en la nube: **ingesta**, **limpieza** y **enriquecimiento** de datos. Utiliza `SQLite` como base de datos local simulando un almacÃ©n analÃ­tico, y `Python` como lenguaje principal para el procesamiento de datos. El flujo de trabajo se automatiza mediante `GitHub Actions` y se documenta con `MkDocs`.

---

### 2. Diagramas de Flujo y Arquitectura

**Diagrama 1: Flujo General del Proyecto**
- Desde la API pÃºblica `restcountries.com`, se extrae la informaciÃ³n de paÃ­ses.
- Los datos son almacenados en SQLite (`countries`).
- Luego se realiza la limpieza y se guarda en `countries_clean`.
- Finalmente, se enriquecen con coordenadas geogrÃ¡ficas usando la API de OpenCage y se guarda en `countries_enriched`.

**Diagrama 2: Proceso de Enriquecimiento**
- Entrada: `cleaning.xlsx` â†’ Proceso de geolocalizaciÃ³n por capital â†’ Salida: `enriched_data.xlsx` y `countries_enriched`.

âœ… *Puedo ayudarte a generar estos diagramas con base en esta estructura si lo deseas.*

---

### 3. ExplicaciÃ³n Detallada de Componentes

#### ðŸ”¹ Ingesta
- **Fuente**: API `https://restcountries.com/v3.1/all`
- **Herramientas**: `requests`, `pandas`, `sqlite3`
- **Salida**: `ingestion.db`, `ingestion.csv`, `ingestion.xlsx`, `ingestion_report.txt`

#### ðŸ”¹ Limpieza
- Elimina duplicados y rellena nulos segÃºn el tipo de dato.
- Guarda resultados en `cleaning.xlsx` y tabla `countries_clean`.

#### ðŸ”¹ Enriquecimiento
- Uso de API `OpenCageData` para agregar latitud y longitud por capital.
- Resultado: tabla `countries_enriched` y Excel `enriched_data.xlsx`.

---

### 4. Modelo de Datos

**Tablas:**
- `countries`: datos en crudo.
- `countries_clean`: despuÃ©s de limpieza.
- `countries_enriched`: con latitud y longitud.

**Relaciones:**
- Todas las tablas comparten el campo clave `pais`.


---

### 5. JustificaciÃ³n de Herramientas

- **SQLite**: ligero, ideal para simulaciÃ³n local y consultas analÃ­ticas.
- **Pandas**: procesamiento flexible y potente de datos.
- **Requests**: extracciÃ³n directa desde APIs REST.
- **GitHub Actions**: permite automatizaciÃ³n continua del flujo ETL.
- **MkDocs**: ideal para documentaciÃ³n estructurada del proyecto.

---

### 6. SimulaciÃ³n del Entorno en la Nube

Aunque no se ejecuta en una nube real como AWS o GCP, el entorno simulado replica sus fases:
- Almacenamiento en SQLite como si fuera un almacÃ©n analÃ­tico.
- AutomatizaciÃ³n y despliegue mediante GitHub Actions.
- VisualizaciÃ³n estructurada con MkDocs como portal web de documentaciÃ³n.

---

### 7. Flujo de Datos y AutomatizaciÃ³n

Desde `main.py` o `run.py`, puede ejecutarse el pipeline completo:
1. `ingestion.py` â†’ genera `countries`
2. `cleaning.py` â†’ transforma en `countries_clean`
3. `enrichment.py` â†’ finaliza en `countries_enriched`

Cada script genera reportes de auditorÃ­a para trazabilidad.

---

### 8. Conclusiones y Recomendaciones

**Beneficios:**
- Flujo estructurado y automatizado.
- Datos enriquecidos para anÃ¡lisis geogrÃ¡fico.
- DocumentaciÃ³n reproducible y clara.

**Limitaciones:**
- No hay paralelizaciÃ³n ni procesamiento distribuido (sin Spark en producciÃ³n).
- SimulaciÃ³n local, no cloud-native real.

**Recomendaciones:**
- Implementar el mismo flujo en AWS con S3 + Glue + Athena.
- Agregar validaciones mÃ¡s complejas.
- Considerar Spark para procesamiento a gran escala.

---

### ðŸ”¥ **Funcionalidades:**

âœ… Realiza una solicitud `GET` para obtener datos de todos los paÃ­ses del mundo.

âœ… Almacena los datos en una base de datos SQLite.

âœ… Exporta los datos a formatos **CSV** y **Excel** para su anÃ¡lisis.

âœ… Limpia los datos, manejando valores nulos y datos duplicados.

âœ… Enriquecer cada paÃ­s con su latitud y longitud usando la API de OpenCage.

âœ… Genera informes de auditorÃ­a detallados con el estado de cada proceso.

---

## ðŸ“‚ **Estructura del Proyecto**

```
[bigdata-infrastructure]
â”‚   README.md
â”‚   requirements.txt
â”‚   run.py
â”‚
â”œâ”€â”€â”€docs
â”‚       index.md
â”‚       ingesta.md
â”‚       limpieza.md
â”‚       enrichment.md
â”‚
â””â”€â”€â”€src
    â”‚   ingestion.py
    â”‚   cleaning.py
    â”‚   enrichment.py
    â”‚
    â””â”€â”€â”€static
        â”œâ”€â”€â”€auditoria
        â”‚       ingestion_report.txt
        â”‚       cleaning_report.txt
        â”‚       enriched_report.txt
        â”œâ”€â”€â”€csv
        â”‚       ingestion.csv
        â”œâ”€â”€â”€db
        â”‚       ingestion.db
        â””â”€â”€â”€xlsx
                ingestion.xlsx
                cleaning.xlsx
                enriched_data.xlsx
```

---

## ðŸ› ï¸ **Requisitos**

Para ejecutar este proyecto necesitas tener instalado:

* **Python 3.x**
* **SQLite3**
* Bibliotecas de Python (ver `requirements.txt`) que incluyen:
  - `pandas`
  - `requests`
  - `openpyxl`
  - `tqdm`

---

## ðŸš€ **EjecuciÃ³n del Proyecto**

Este script ejecuta en orden toda la rutina del programa:

```bash
python run.py
```

---

## ðŸ“Š Modelo de Base de Datos

### ðŸ§© Estructura de las tablas

#### Tabla `countries`

Contiene la informaciÃ³n original obtenida desde la API.

#### Tabla `countries_clean`

Contiene los datos luego de ser limpiados.

#### Tabla `countries_enriched`

Contiene los datos enriquecidos con latitud y longitud.

```sql
CREATE TABLE IF NOT EXISTS countries (
    pais TEXT,
    capital TEXT,
    region TEXT,
    subregion TEXT,
    poblacion INTEGER,
    area REAL
)
```

---

## ðŸ§  Diagrama Mermaid â€“ Flujo General

``` mermaid
flowchart TD
    A[Ingesta API RESTCountries] --> B[Guardar en SQLite y Excel]
    B --> C[Limpieza de Datos]
    C --> D[Guardar limpio en cleaning.xlsx y countries_clean]
    D --> E[Enriquecimiento con OpenCage]
    E --> F[Guardar enriquecido en enriched_data.xlsx y countries_enriched]
```