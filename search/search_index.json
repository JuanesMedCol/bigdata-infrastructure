{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Proyecto Integrador de Infraestructura y Arquitectura para Big Data","text":"<p>Este proyecto est\u00e1 dise\u00f1ado para realizar la ingesta de datos desde una API p\u00fablica, almacenarlos en una base de datos SQLite y permitir su posterior limpieza y an\u00e1lisis mediante scripts automatizados en Python.</p>"},{"location":"index.html#descripcion","title":"\ud83d\ude80 Descripci\u00f3n","text":"<p>Este proyecto obtiene datos de la API JSONPlaceholder, que es un servicio de pruebas para desarrolladores. Los datos extra\u00eddos se almacenan en una base de datos SQLite local, permitiendo su an\u00e1lisis, limpieza y manipulaci\u00f3n posterior.</p>"},{"location":"index.html#funcionalidades","title":"\ud83d\udd25 Funcionalidades:","text":"<p>\u2705 Realiza una solicitud <code>GET</code> a la API para obtener datos.</p> <p>\u2705 Almacena los datos en una base de datos SQLite.</p> <p>\u2705 Evita duplicados en la base de datos mediante restricciones \u00fanicas.</p> <p>\u2705 Exporta los datos a formatos CSV y Excel para su an\u00e1lisis.</p> <p>\u2705 Limpia los datos, manejando valores nulos y datos duplicados.</p> <p>\u2705 Genera un informe de auditor\u00eda detallado con el estado de los datos.</p>"},{"location":"index.html#estructura-del-proyecto","title":"\ud83d\udcc2 Estructura del Proyecto","text":"<pre><code>[bigdata-infrastructure]\n\u2502   .gitattributes\n\u2502   .gitignore\n\u2502   mkdocs.yml\n\u2502   README.md\n\u2502   requirements.txt\n\u2502   run.py\n\u2502   setup.py\n\u2502\n\u251c\u2500\u2500\u2500.github\n\u2502   \u2514\u2500\u2500\u2500workflows\n\u2502           bigdata.yml\n\u2502\n\u251c\u2500\u2500\u2500.qodo\n\u2502       history.sqlite\n\u2502\n\u251c\u2500\u2500\u2500bigdata_infrastructure.egg-info\n\u2502       dependency_links.txt\n\u2502       PKG-INFO\n\u2502       requires.txt\n\u2502       SOURCES.txt\n\u2502       top_level.txt\n\u2502\n\u251c\u2500\u2500\u2500build\n\u2502   \u2514\u2500\u2500\u2500bdist.win-amd64\n\u251c\u2500\u2500\u2500docs\n\u2502       index.md\n\u2502       ingesta.md\n\u2502       limpieza.md\n\u2502\n\u2514\u2500\u2500\u2500src\n    \u2502   cleaning.py\n    \u2502   ingestion.py\n    \u2502\n    \u2514\u2500\u2500\u2500static\n        \u251c\u2500\u2500\u2500auditoria\n        \u2502       cleaning_report.txt\n        \u2502       ingestion_report.txt\n        \u2502\n        \u251c\u2500\u2500\u2500csv\n        \u2502       ingestion.csv\n        \u2502\n        \u251c\u2500\u2500\u2500db\n        \u2502       ingestion.db\n        \u2502\n        \u2514\u2500\u2500\u2500xlsx\n                cleaning.xlsx\n                ingestion.xlsx\n</code></pre>"},{"location":"index.html#requisitos","title":"\ud83d\udee0\ufe0f Requisitos","text":"<p>Para ejecutar este proyecto, necesitas tener instalados los siguientes paquetes y herramientas:</p> <ul> <li>Python 3.x</li> <li>SQLite3 (ya viene incluido con Python)</li> <li>Bibliotecas adicionales de Python que puedes instalar f\u00e1cilmente usando <code>pip</code></li> </ul>"},{"location":"index.html#instalacion-de-las-dependencias","title":"\ud83d\udce5 Instalaci\u00f3n de las dependencias","text":"<ol> <li>Clona el repositorio desde GitHub:</li> </ol> <pre><code>git clone https://github.com/JuanesMedCol/bigdata-infrastructure.git\n</code></pre> <ol> <li>Accede al directorio del proyecto:</li> </ol> <pre><code>cd bigdata-infrastructure\n</code></pre> <ol> <li>Instala las dependencias desde el archivo <code>requirements.txt</code> o usando el setup:</li> </ol> <pre><code>Opcion 1: pip install -r requirements.txt\nOpcion 2: pip install .\n</code></pre>"},{"location":"index.html#ejecucion-del-proyecto","title":"\ud83d\ude80 Ejecuci\u00f3n del Proyecto","text":"<p>Este script ejecuta en orden toda la rutina del programa:</p> <pre><code>python run.py\n</code></pre>"},{"location":"index.html#modelo-de-base-de-datos","title":"\ud83d\udcca Modelo de Base de Datos","text":""},{"location":"index.html#estructura-de-la-tabla-posts","title":"\ud83e\udde9 Estructura de la tabla <code>posts</code>","text":"<p>La base de datos <code>ingestion.db</code> contiene la tabla <code>posts</code> donde se almacenan los datos obtenidos del API.</p> <pre><code>CREATE TABLE IF NOT EXISTS posts (\n    id INTEGER PRIMARY KEY,\n    title TEXT,\n    body TEXT\n)\n</code></pre>"},{"location":"index.html#diagrama-mermaid-modelo-de-datos","title":"\ud83e\udde0 Diagrama Mermaid \u2013 Modelo de Datos","text":"<pre><code>erDiagram\n    posts {\n        INTEGER id PK \"Identificador \u00fanico del post\"\n        TEXT title \"T\u00edtulo del post\"\n        TEXT body \"Contenido del post\"\n    }\n</code></pre>"},{"location":"ingesta.html","title":"Ingesta de Datos","text":""},{"location":"ingesta.html#ingesta-de-datos-desde-api-rest","title":"\ud83d\udcc4 Ingesta de Datos desde API REST","text":"<p>Este script realiza la ingesta de datos desde un endpoint p\u00fablico (<code>https://jsonplaceholder.typicode.com/posts</code>) y los procesa en diferentes formatos. A continuaci\u00f3n, se documenta cada etapa del flujo.</p>"},{"location":"ingesta.html#inicio-del-proceso","title":"\ud83d\ude80 Inicio del Proceso","text":"<ul> <li>Se establece la configuraci\u00f3n de <code>logging</code> para mostrar mensajes en consola.</li> <li>Se define la URL de la API.</li> </ul> <pre><code>url = 'https://jsonplaceholder.typicode.com/posts'\n</code></pre>"},{"location":"ingesta.html#consulta-a-la-api","title":"\ud83c\udf10 Consulta a la API","text":"<ul> <li>Se hace una solicitud <code>GET</code> a la API.</li> <li>Si la respuesta es exitosa (<code>status_code == 200</code>), se procesa la informaci\u00f3n.</li> <li>En caso de error, el script finaliza.</li> </ul>"},{"location":"ingesta.html#rutas-y-directorios","title":"\ud83d\udcc1 Rutas y Directorios","text":"<p>El script crea las siguientes rutas para guardar los datos:</p> <ul> <li><code>src/static/db/ingestion.db</code>: Base de datos SQLite</li> <li><code>src/static/csv/ingestion.csv</code>: Archivo CSV</li> <li><code>src/static/xlsx/ingestion.xlsx</code>: Archivo Excel</li> <li><code>src/static/auditoria/ingestion_report.txt</code>: Reporte de auditor\u00eda</li> </ul> <p>Se crean las carpetas autom\u00e1ticamente si no existen.</p>"},{"location":"ingesta.html#almacenamiento-en-base-de-datos","title":"\ud83d\uddc3\ufe0f Almacenamiento en Base de Datos","text":"<p>Se conecta a una base de datos SQLite y crea una tabla llamada <code>posts</code> con las siguientes columnas:</p> Campo Tipo id INTEGER title TEXT body TEXT <p>Luego se insertan los datos obtenidos desde el API.</p>"},{"location":"ingesta.html#exportacion-a-formatos","title":"\ud83d\udce4 Exportaci\u00f3n a Formatos","text":"<p>Se exportan los primeros 10 registros desde la base de datos a:</p> <ul> <li>CSV : <code>src/static/csv/ingestion.csv</code></li> <li>Excel : <code>src/static/xlsx/ingestion.xlsx</code></li> </ul>"},{"location":"ingesta.html#auditoria-de-datos","title":"\ud83d\udd75\ufe0f Auditor\u00eda de Datos","text":"<p>El script verifica que los datos guardados coincidan con los datos obtenidos del API. Para cada registro:</p> <ul> <li>\u2714\ufe0f Si el <code>title</code> y <code>body</code> coinciden con la base de datos \u2192 se cuenta como \"coincidente\".</li> <li>\u274c Si no hay coincidencia \u2192 se escribe en el archivo <code>ingestion_report.txt</code>.</li> </ul>"},{"location":"ingesta.html#resumen-generado","title":"\ud83d\udcca Resumen generado:","text":"<pre><code>\u2714\ufe0f Registros coincidentes: X\n\u274c Registros no encontrados: Y\n</code></pre>"},{"location":"ingesta.html#fin-del-proceso","title":"\u2705 Fin del Proceso","text":"<p>El proceso termina con un mensaje de \u00e9xito:</p> <pre><code>Proceso finalizado correctamente \u2705\n</code></pre>"},{"location":"ingesta.html#archivos-descargables","title":"\ud83d\udcce Archivos descargables","text":"<ul> <li>CSV de Ingesta</li> <li>Excel de Ingesta</li> <li>Reporte de Auditor\u00eda</li> </ul>"},{"location":"ingesta.html#diagrama-del-proceso-de-ingesta","title":"\ud83e\udde0 Diagrama del Proceso de Ingesta","text":"<pre><code>flowchart TD\n    A[Inicio del proceso] --&gt; B[Llamada a la API externa (GET /posts)]\n    B --&gt; C{\u00bfRespuesta 200 OK?}\n    C -- S\u00ed --&gt; D[Extraer datos en formato JSON]\n    C -- No --&gt; Z[Terminar con error]\n\n    D --&gt; E[Crear carpetas de salida si no existen]\n    E --&gt; F[Conectar a base de datos SQLite (ingestion.db)]\n    F --&gt; G[Crear tabla posts si no existe]\n    G --&gt; H[Insertar o reemplazar registros en la tabla]\n\n    H --&gt; I[Cerrar conexi\u00f3n y confirmar datos insertados]\n    I --&gt; J[Reabrir conexi\u00f3n y leer 10 registros con Pandas]\n    J --&gt; K[Exportar a CSV]\n    J --&gt; L[Exportar a Excel]\n\n    K --&gt; M[Iniciar generaci\u00f3n del informe de auditor\u00eda]\n    L --&gt; M\n\n    M --&gt; N[Comparar datos extra\u00eddos vs base de datos]\n    N --&gt; O[Contar coincidencias y errores]\n    O --&gt; P[Escribir archivo ingestion_report.txt]\n\n    P --&gt; Q[Finalizar proceso con log de \u00e9xito \u2705]\n</code></pre>"},{"location":"limpieza.html","title":"Limpieza de Datos","text":""},{"location":"limpieza.html#limpieza-y-transformacion-de-datos","title":"\ud83e\uddfc Limpieza y Transformaci\u00f3n de Datos","text":"<p>Este script realiza una limpieza autom\u00e1tica de datos almacenados en una base de datos SQLite. A continuaci\u00f3n, se detalla cada una de las etapas del proceso.</p>"},{"location":"limpieza.html#estructura-de-archivos","title":"\ud83d\udcc1 Estructura de Archivos","text":"<ul> <li>Base de datos de entrada : <code>src/static/db/ingestion.db</code></li> <li>Archivo Excel de salida : <code>src/static/xlsx/cleaning.xlsx</code></li> <li>Reporte de auditor\u00eda : <code>src/static/auditoria/cleaning_report.txt</code></li> </ul>"},{"location":"limpieza.html#1-carga-de-datos","title":"\ud83d\udd39 1. Carga de Datos","text":"<p>Se conecta a la base de datos SQLite y se lee la tabla <code>posts</code>.</p> <pre><code>def load_data():\n    conn = sqlite3.connect(DB_PATH)\n    query = \"SELECT * FROM posts\"\n    df = pd.read_sql_query(query, conn)\n</code></pre>"},{"location":"limpieza.html#2-analisis-exploratorio","title":"\ud83d\udd0d 2. An\u00e1lisis Exploratorio","text":"<p>Se genera un peque\u00f1o resumen con:</p> <ul> <li>N\u00famero total de registros</li> <li>Cantidad de valores nulos por columna</li> <li>Registros duplicados</li> </ul> <pre><code>def explore_data(df):\n    df.isnull().sum()\n    df.duplicated().sum()\n</code></pre>"},{"location":"limpieza.html#3-proceso-de-limpieza","title":"\ud83e\uddfc 3. Proceso de Limpieza","text":"<p>La limpieza incluye:</p> <ul> <li>\u2705 Eliminaci\u00f3n de duplicados</li> <li>\u2705 Relleno de valores nulos:</li> <li>Num\u00e9ricos \u2192 con la media</li> <li>Texto \u2192 con el valor anterior (<code>ffill</code>)</li> <li>\u2705 Conversi\u00f3n de tipos:</li> <li><code>fecha</code> \u2192 datetime</li> <li><code>monto</code> \u2192 num\u00e9rico</li> <li>Normalizaci\u00f3n de <code>monto</code> (media 0, desviaci\u00f3n est\u00e1ndar 1)</li> </ul> <pre><code>df = df.drop_duplicates()\ndf[col] = df[col].fillna(df[col].mean())  # para columnas num\u00e9ricas\ndf[col] = df[col].ffill()                 # para texto\n</code></pre>"},{"location":"limpieza.html#4-guardado-de-resultados","title":"\ud83d\udcbe 4. Guardado de Resultados","text":"<p>Se exportan los datos a:</p> <ul> <li>Archivo Excel (<code>.xlsx</code>)</li> <li>Un reporte de limpieza en <code>.txt</code>, que incluye resumen del an\u00e1lisis y total de registros finales</li> </ul> <pre><code>df.to_excel(OUTPUT_CSV, index=False)\nwith open(AUDIT_FILE, \"w\", encoding=\"utf-8\") as f:\n    f.write(...)\n</code></pre>"},{"location":"limpieza.html#reporte-generado","title":"\ud83d\udcca Reporte generado","text":"<p>Ejemplo de contenido del <code>cleaning_report.txt</code>:</p> <pre><code>\ud83d\udccb Informe de Limpieza de Datos\n\nN\u00famero total de registros: 100\nN\u00famero de valores nulos por columna:\nid       0\ntitle    0\nbody     0\n...\nN\u00famero de registros duplicados: 2\n\n\u2705 N\u00famero de registros despu\u00e9s de limpieza: 98\n</code></pre>"},{"location":"limpieza.html#resultado-final","title":"\u2705 Resultado Final","text":"<p>El script completa el proceso con logs informativos:</p> <pre><code>\ud83d\ude80 Inicio del proceso de limpieza de datos\n\ud83d\udce5 Datos cargados correctamente...\n\u2705 Proceso completado con \u00e9xito.\n</code></pre>"},{"location":"limpieza.html#archivos-generados","title":"\ud83d\udcce Archivos Generados","text":"<ul> <li>Excel de Limpieza</li> <li>Reporte de Auditoria</li> </ul>"},{"location":"limpieza.html#diagrama-del-flujo-de-limpieza","title":"\ud83e\udde0 Diagrama del Flujo de Limpieza","text":"<pre><code>flowchart TD\n    A[Inicio del Proceso] --&gt; B[Cargar base de datos (ingestion.db)]\n    B --&gt; C[Leer tabla \"posts\" con Pandas]\n    C --&gt; D[An\u00e1lisis exploratorio]\n    D --&gt; D1[Contar registros]\n    D --&gt; D2[Buscar nulos]\n    D --&gt; D3[Buscar duplicados]\n\n    D3 --&gt; E[Proceso de limpieza]\n    E --&gt; E1[Eliminar duplicados]\n    E --&gt; E2[Rellenar nulos]\n    E2 --&gt; E2a[Num\u00e9ricos \u2192 media]\n    E2 --&gt; E2b[Texto \u2192 ffill()]\n    E --&gt; E3[Conversi\u00f3n de tipos]\n    E3 --&gt; E3a[fecha \u2192 datetime]\n    E3 --&gt; E3b[monto \u2192 num\u00e9rico + normalizaci\u00f3n]\n\n    E3b --&gt; F[Verificar limpieza]\n    F --&gt; G[Exportar a Excel (cleaning.xlsx)]\n    F --&gt; H[Generar reporte de limpieza (cleaning_report.txt)]\n\n    G --&gt; I[Fin del proceso]\n    H --&gt; I\n</code></pre>"}]}